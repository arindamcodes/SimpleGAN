{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, img_dim):\n",
    "\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(img_dim, 128)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(z_dim, 256)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.fc2 = nn.Linear(256, img_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "lr = 1e-4\n",
    "z_dim = 64\n",
    "img_dim = 28 * 28\n",
    "batch_size = 32\n",
    "epochs = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Discriminator and Generator\n",
    "dis= Discriminator(img_dim).to(device)\n",
    "gen = Generator(z_dim, img_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation\n",
    "transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5, ))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using MNIST dataset\n",
    "\n",
    "dataset = datasets.MNIST(root=\"datasets/\", \n",
    "                         transform=transforms, \n",
    "                         download=False)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "opt_dis = optim.Adam(dis.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "writer_fake = SummaryWriter(f\"runs/GAN_MNIST/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/GAN_MNIST/real\")\n",
    "step = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arindams_mac_m2_pro/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200] Batch 0/1875                       Loss D: 43.1238, loss G: -23.0622\n",
      "Epoch [1/200] Batch 0/1875                       Loss D: 13.8071, loss G: -7.0354\n",
      "Epoch [2/200] Batch 0/1875                       Loss D: 40.0646, loss G: -18.2839\n",
      "Epoch [3/200] Batch 0/1875                       Loss D: 25.9287, loss G: -12.4513\n",
      "Epoch [4/200] Batch 0/1875                       Loss D: 21.2249, loss G: -9.6153\n",
      "Epoch [5/200] Batch 0/1875                       Loss D: 35.9114, loss G: -16.7880\n",
      "Epoch [6/200] Batch 0/1875                       Loss D: 48.4230, loss G: -25.9472\n",
      "Epoch [7/200] Batch 0/1875                       Loss D: 52.0968, loss G: -23.8745\n",
      "Epoch [8/200] Batch 0/1875                       Loss D: 12.1634, loss G: -5.3786\n",
      "Epoch [9/200] Batch 0/1875                       Loss D: 11.8042, loss G: -6.3946\n",
      "Epoch [10/200] Batch 0/1875                       Loss D: 21.8777, loss G: -10.1736\n",
      "Epoch [11/200] Batch 0/1875                       Loss D: 12.2278, loss G: -5.4694\n",
      "Epoch [12/200] Batch 0/1875                       Loss D: 51.7683, loss G: -19.9583\n",
      "Epoch [13/200] Batch 0/1875                       Loss D: 1.8162, loss G: -0.8498\n",
      "Epoch [14/200] Batch 0/1875                       Loss D: 16.8458, loss G: -8.9519\n",
      "Epoch [15/200] Batch 0/1875                       Loss D: 10.3856, loss G: -5.6717\n",
      "Epoch [16/200] Batch 0/1875                       Loss D: 27.3544, loss G: -11.7920\n",
      "Epoch [17/200] Batch 0/1875                       Loss D: 32.7892, loss G: -8.3610\n",
      "Epoch [18/200] Batch 0/1875                       Loss D: 43.2738, loss G: -19.9146\n",
      "Epoch [19/200] Batch 0/1875                       Loss D: 31.5487, loss G: -11.8086\n",
      "Epoch [20/200] Batch 0/1875                       Loss D: 59.0342, loss G: -24.7329\n",
      "Epoch [21/200] Batch 0/1875                       Loss D: 25.0455, loss G: -10.9616\n",
      "Epoch [22/200] Batch 0/1875                       Loss D: 21.9307, loss G: -4.6975\n",
      "Epoch [23/200] Batch 0/1875                       Loss D: 12.9489, loss G: -7.0418\n",
      "Epoch [24/200] Batch 0/1875                       Loss D: 24.7300, loss G: -9.3101\n",
      "Epoch [25/200] Batch 0/1875                       Loss D: 29.2184, loss G: -15.3682\n",
      "Epoch [26/200] Batch 0/1875                       Loss D: 23.1892, loss G: -6.9357\n",
      "Epoch [27/200] Batch 0/1875                       Loss D: 31.9761, loss G: -12.5195\n",
      "Epoch [28/200] Batch 0/1875                       Loss D: 26.8470, loss G: -12.2617\n",
      "Epoch [29/200] Batch 0/1875                       Loss D: 23.1719, loss G: -12.0896\n",
      "Epoch [30/200] Batch 0/1875                       Loss D: 30.1635, loss G: -16.6218\n",
      "Epoch [31/200] Batch 0/1875                       Loss D: 29.8416, loss G: -12.3910\n",
      "Epoch [32/200] Batch 0/1875                       Loss D: 40.7858, loss G: -17.7008\n",
      "Epoch [33/200] Batch 0/1875                       Loss D: 11.5633, loss G: -6.5014\n",
      "Epoch [34/200] Batch 0/1875                       Loss D: 38.1316, loss G: -18.5818\n",
      "Epoch [35/200] Batch 0/1875                       Loss D: 22.7504, loss G: -11.2172\n",
      "Epoch [36/200] Batch 0/1875                       Loss D: 53.0757, loss G: -24.8014\n",
      "Epoch [37/200] Batch 0/1875                       Loss D: 45.0701, loss G: -14.3392\n",
      "Epoch [38/200] Batch 0/1875                       Loss D: 28.1234, loss G: -10.5509\n",
      "Epoch [39/200] Batch 0/1875                       Loss D: 48.7604, loss G: -22.2861\n",
      "Epoch [40/200] Batch 0/1875                       Loss D: 19.2690, loss G: -8.8133\n",
      "Epoch [41/200] Batch 0/1875                       Loss D: 30.0397, loss G: -14.8860\n",
      "Epoch [42/200] Batch 0/1875                       Loss D: 25.0620, loss G: -10.3638\n",
      "Epoch [43/200] Batch 0/1875                       Loss D: 23.2715, loss G: -11.3079\n",
      "Epoch [44/200] Batch 0/1875                       Loss D: 29.5394, loss G: -12.6621\n",
      "Epoch [45/200] Batch 0/1875                       Loss D: 37.6934, loss G: -15.2964\n",
      "Epoch [46/200] Batch 0/1875                       Loss D: 54.7614, loss G: -21.3463\n",
      "Epoch [47/200] Batch 0/1875                       Loss D: 44.1054, loss G: -19.6274\n",
      "Epoch [48/200] Batch 0/1875                       Loss D: 54.3107, loss G: -23.0343\n",
      "Epoch [49/200] Batch 0/1875                       Loss D: 34.0229, loss G: -16.4359\n",
      "Epoch [50/200] Batch 0/1875                       Loss D: 28.1402, loss G: -10.9535\n",
      "Epoch [51/200] Batch 0/1875                       Loss D: 36.1175, loss G: -16.8059\n",
      "Epoch [52/200] Batch 0/1875                       Loss D: 31.8625, loss G: -15.1321\n",
      "Epoch [53/200] Batch 0/1875                       Loss D: 30.2500, loss G: -13.3103\n",
      "Epoch [54/200] Batch 0/1875                       Loss D: 34.7093, loss G: -14.7299\n",
      "Epoch [55/200] Batch 0/1875                       Loss D: 28.9595, loss G: -13.0870\n",
      "Epoch [56/200] Batch 0/1875                       Loss D: 37.8254, loss G: -9.2036\n",
      "Epoch [57/200] Batch 0/1875                       Loss D: 35.9514, loss G: -14.9648\n",
      "Epoch [58/200] Batch 0/1875                       Loss D: 34.2707, loss G: -15.4857\n",
      "Epoch [59/200] Batch 0/1875                       Loss D: 30.6864, loss G: -14.0760\n",
      "Epoch [60/200] Batch 0/1875                       Loss D: 37.7264, loss G: -18.3552\n",
      "Epoch [61/200] Batch 0/1875                       Loss D: 31.1013, loss G: -14.9607\n",
      "Epoch [62/200] Batch 0/1875                       Loss D: 28.0836, loss G: -13.4371\n",
      "Epoch [63/200] Batch 0/1875                       Loss D: 34.5210, loss G: -17.6988\n",
      "Epoch [64/200] Batch 0/1875                       Loss D: 36.0808, loss G: -18.6108\n",
      "Epoch [65/200] Batch 0/1875                       Loss D: 53.9076, loss G: -26.8298\n",
      "Epoch [66/200] Batch 0/1875                       Loss D: 33.8359, loss G: -18.0178\n",
      "Epoch [67/200] Batch 0/1875                       Loss D: 35.8728, loss G: -17.7956\n",
      "Epoch [68/200] Batch 0/1875                       Loss D: 36.8400, loss G: -15.3249\n",
      "Epoch [69/200] Batch 0/1875                       Loss D: 44.4461, loss G: -25.9234\n",
      "Epoch [70/200] Batch 0/1875                       Loss D: 38.4636, loss G: -14.1788\n",
      "Epoch [71/200] Batch 0/1875                       Loss D: 33.8732, loss G: -14.2161\n",
      "Epoch [72/200] Batch 0/1875                       Loss D: 36.3007, loss G: -17.0007\n",
      "Epoch [73/200] Batch 0/1875                       Loss D: 45.7797, loss G: -18.0377\n",
      "Epoch [74/200] Batch 0/1875                       Loss D: 48.0065, loss G: -21.1545\n",
      "Epoch [75/200] Batch 0/1875                       Loss D: 39.3728, loss G: -18.2063\n",
      "Epoch [76/200] Batch 0/1875                       Loss D: 36.9272, loss G: -16.5921\n",
      "Epoch [77/200] Batch 0/1875                       Loss D: 37.8516, loss G: -20.8256\n",
      "Epoch [78/200] Batch 0/1875                       Loss D: 40.1052, loss G: -18.6543\n",
      "Epoch [79/200] Batch 0/1875                       Loss D: 37.1304, loss G: -18.8451\n",
      "Epoch [80/200] Batch 0/1875                       Loss D: 43.7852, loss G: -20.9072\n",
      "Epoch [81/200] Batch 0/1875                       Loss D: 40.6054, loss G: -14.2414\n",
      "Epoch [82/200] Batch 0/1875                       Loss D: 43.1112, loss G: -17.4858\n",
      "Epoch [83/200] Batch 0/1875                       Loss D: 37.1579, loss G: -17.0668\n",
      "Epoch [84/200] Batch 0/1875                       Loss D: 36.4293, loss G: -19.7024\n",
      "Epoch [85/200] Batch 0/1875                       Loss D: 39.2520, loss G: -20.2656\n",
      "Epoch [86/200] Batch 0/1875                       Loss D: 39.7284, loss G: -21.2670\n",
      "Epoch [87/200] Batch 0/1875                       Loss D: 34.2948, loss G: -15.9399\n",
      "Epoch [88/200] Batch 0/1875                       Loss D: 34.7314, loss G: -18.8383\n",
      "Epoch [89/200] Batch 0/1875                       Loss D: 41.5876, loss G: -22.7429\n",
      "Epoch [90/200] Batch 0/1875                       Loss D: 38.8676, loss G: -19.2734\n",
      "Epoch [91/200] Batch 0/1875                       Loss D: 38.3753, loss G: -18.6391\n",
      "Epoch [92/200] Batch 0/1875                       Loss D: 28.5342, loss G: -18.4691\n",
      "Epoch [93/200] Batch 0/1875                       Loss D: 36.7384, loss G: -16.6425\n",
      "Epoch [94/200] Batch 0/1875                       Loss D: 45.2689, loss G: -16.5148\n",
      "Epoch [95/200] Batch 0/1875                       Loss D: 36.8981, loss G: -15.0951\n",
      "Epoch [96/200] Batch 0/1875                       Loss D: 30.4597, loss G: -16.0665\n",
      "Epoch [97/200] Batch 0/1875                       Loss D: 40.9017, loss G: -19.5884\n",
      "Epoch [98/200] Batch 0/1875                       Loss D: 38.8814, loss G: -21.3908\n",
      "Epoch [99/200] Batch 0/1875                       Loss D: 34.8848, loss G: -19.5495\n",
      "Epoch [100/200] Batch 0/1875                       Loss D: 33.4741, loss G: -15.7984\n",
      "Epoch [101/200] Batch 0/1875                       Loss D: 36.3305, loss G: -19.0063\n",
      "Epoch [102/200] Batch 0/1875                       Loss D: 37.5028, loss G: -18.6705\n",
      "Epoch [103/200] Batch 0/1875                       Loss D: 35.4150, loss G: -16.8658\n",
      "Epoch [104/200] Batch 0/1875                       Loss D: 41.5082, loss G: -19.8160\n",
      "Epoch [105/200] Batch 0/1875                       Loss D: 39.4033, loss G: -17.9924\n",
      "Epoch [106/200] Batch 0/1875                       Loss D: 33.1609, loss G: -13.7785\n",
      "Epoch [107/200] Batch 0/1875                       Loss D: 34.3400, loss G: -16.5205\n",
      "Epoch [108/200] Batch 0/1875                       Loss D: 45.7666, loss G: -22.2548\n",
      "Epoch [109/200] Batch 0/1875                       Loss D: 38.6767, loss G: -20.5317\n",
      "Epoch [110/200] Batch 0/1875                       Loss D: 39.2339, loss G: -23.2072\n",
      "Epoch [111/200] Batch 0/1875                       Loss D: 35.4937, loss G: -16.8079\n",
      "Epoch [112/200] Batch 0/1875                       Loss D: 36.9791, loss G: -16.1744\n",
      "Epoch [113/200] Batch 0/1875                       Loss D: 31.3121, loss G: -13.2951\n",
      "Epoch [114/200] Batch 0/1875                       Loss D: 34.6932, loss G: -16.5329\n",
      "Epoch [115/200] Batch 0/1875                       Loss D: 39.7030, loss G: -19.7208\n",
      "Epoch [116/200] Batch 0/1875                       Loss D: 32.1813, loss G: -18.3198\n",
      "Epoch [117/200] Batch 0/1875                       Loss D: 34.0005, loss G: -19.0314\n",
      "Epoch [118/200] Batch 0/1875                       Loss D: 39.3444, loss G: -14.6085\n",
      "Epoch [119/200] Batch 0/1875                       Loss D: 34.6721, loss G: -18.3750\n",
      "Epoch [120/200] Batch 0/1875                       Loss D: 36.1232, loss G: -20.0362\n",
      "Epoch [121/200] Batch 0/1875                       Loss D: 39.2150, loss G: -17.3716\n",
      "Epoch [122/200] Batch 0/1875                       Loss D: 36.6603, loss G: -18.1361\n",
      "Epoch [123/200] Batch 0/1875                       Loss D: 33.8931, loss G: -18.5343\n",
      "Epoch [124/200] Batch 0/1875                       Loss D: 30.7533, loss G: -11.6309\n",
      "Epoch [125/200] Batch 0/1875                       Loss D: 34.0311, loss G: -14.3744\n",
      "Epoch [126/200] Batch 0/1875                       Loss D: 30.9735, loss G: -14.1415\n",
      "Epoch [127/200] Batch 0/1875                       Loss D: 36.4988, loss G: -15.6774\n",
      "Epoch [128/200] Batch 0/1875                       Loss D: 39.6708, loss G: -20.5640\n",
      "Epoch [129/200] Batch 0/1875                       Loss D: 39.4337, loss G: -23.3538\n",
      "Epoch [130/200] Batch 0/1875                       Loss D: 36.6312, loss G: -18.5459\n",
      "Epoch [131/200] Batch 0/1875                       Loss D: 33.4197, loss G: -14.8501\n",
      "Epoch [132/200] Batch 0/1875                       Loss D: 34.8082, loss G: -18.6759\n",
      "Epoch [133/200] Batch 0/1875                       Loss D: 33.8628, loss G: -13.6998\n",
      "Epoch [134/200] Batch 0/1875                       Loss D: 36.2063, loss G: -20.3265\n",
      "Epoch [135/200] Batch 0/1875                       Loss D: 39.7331, loss G: -15.1785\n",
      "Epoch [136/200] Batch 0/1875                       Loss D: 34.0639, loss G: -13.0836\n",
      "Epoch [137/200] Batch 0/1875                       Loss D: 37.8395, loss G: -19.4116\n",
      "Epoch [138/200] Batch 0/1875                       Loss D: 39.4122, loss G: -24.8825\n",
      "Epoch [139/200] Batch 0/1875                       Loss D: 39.2822, loss G: -25.1822\n",
      "Epoch [140/200] Batch 0/1875                       Loss D: 35.0018, loss G: -16.4405\n",
      "Epoch [141/200] Batch 0/1875                       Loss D: 30.4818, loss G: -14.0733\n",
      "Epoch [142/200] Batch 0/1875                       Loss D: 36.3510, loss G: -17.0707\n",
      "Epoch [143/200] Batch 0/1875                       Loss D: 44.8206, loss G: -20.6427\n",
      "Epoch [144/200] Batch 0/1875                       Loss D: 31.7412, loss G: -15.0702\n",
      "Epoch [145/200] Batch 0/1875                       Loss D: 32.8492, loss G: -13.7325\n",
      "Epoch [146/200] Batch 0/1875                       Loss D: 35.0496, loss G: -18.5927\n",
      "Epoch [147/200] Batch 0/1875                       Loss D: 35.0389, loss G: -14.9150\n",
      "Epoch [148/200] Batch 0/1875                       Loss D: 30.0459, loss G: -15.3152\n",
      "Epoch [149/200] Batch 0/1875                       Loss D: 37.9878, loss G: -17.5558\n",
      "Epoch [150/200] Batch 0/1875                       Loss D: 37.3177, loss G: -19.7778\n",
      "Epoch [151/200] Batch 0/1875                       Loss D: 35.0336, loss G: -15.0436\n",
      "Epoch [152/200] Batch 0/1875                       Loss D: 40.8091, loss G: -22.0250\n",
      "Epoch [153/200] Batch 0/1875                       Loss D: 35.6491, loss G: -18.3495\n",
      "Epoch [154/200] Batch 0/1875                       Loss D: 36.1826, loss G: -18.3697\n",
      "Epoch [155/200] Batch 0/1875                       Loss D: 34.6856, loss G: -15.1823\n",
      "Epoch [156/200] Batch 0/1875                       Loss D: 36.1684, loss G: -20.0724\n",
      "Epoch [157/200] Batch 0/1875                       Loss D: 31.1800, loss G: -14.5641\n",
      "Epoch [158/200] Batch 0/1875                       Loss D: 31.7951, loss G: -14.1809\n",
      "Epoch [159/200] Batch 0/1875                       Loss D: 35.0335, loss G: -15.8262\n",
      "Epoch [160/200] Batch 0/1875                       Loss D: 30.2474, loss G: -13.5623\n",
      "Epoch [161/200] Batch 0/1875                       Loss D: 42.0357, loss G: -16.5356\n",
      "Epoch [162/200] Batch 0/1875                       Loss D: 36.4864, loss G: -20.4643\n",
      "Epoch [163/200] Batch 0/1875                       Loss D: 34.2306, loss G: -17.4925\n",
      "Epoch [164/200] Batch 0/1875                       Loss D: 29.6605, loss G: -15.4391\n",
      "Epoch [165/200] Batch 0/1875                       Loss D: 31.6901, loss G: -15.3425\n",
      "Epoch [166/200] Batch 0/1875                       Loss D: 41.3357, loss G: -20.5109\n",
      "Epoch [167/200] Batch 0/1875                       Loss D: 32.1575, loss G: -15.3225\n",
      "Epoch [168/200] Batch 0/1875                       Loss D: 30.3620, loss G: -13.6288\n",
      "Epoch [169/200] Batch 0/1875                       Loss D: 36.3875, loss G: -17.7840\n",
      "Epoch [170/200] Batch 0/1875                       Loss D: 36.3692, loss G: -16.4695\n",
      "Epoch [171/200] Batch 0/1875                       Loss D: 29.9886, loss G: -12.4271\n",
      "Epoch [172/200] Batch 0/1875                       Loss D: 30.0228, loss G: -15.4234\n",
      "Epoch [173/200] Batch 0/1875                       Loss D: 38.4805, loss G: -13.8890\n",
      "Epoch [174/200] Batch 0/1875                       Loss D: 26.7146, loss G: -13.0603\n",
      "Epoch [175/200] Batch 0/1875                       Loss D: 37.9323, loss G: -13.1156\n",
      "Epoch [176/200] Batch 0/1875                       Loss D: 33.9662, loss G: -16.8650\n",
      "Epoch [177/200] Batch 0/1875                       Loss D: 30.9918, loss G: -13.9772\n",
      "Epoch [178/200] Batch 0/1875                       Loss D: 35.9156, loss G: -17.9579\n",
      "Epoch [179/200] Batch 0/1875                       Loss D: 36.4385, loss G: -19.0992\n",
      "Epoch [180/200] Batch 0/1875                       Loss D: 36.2372, loss G: -16.6211\n",
      "Epoch [181/200] Batch 0/1875                       Loss D: 32.5329, loss G: -15.1538\n",
      "Epoch [182/200] Batch 0/1875                       Loss D: 37.8111, loss G: -16.9643\n",
      "Epoch [183/200] Batch 0/1875                       Loss D: 35.6732, loss G: -14.7668\n",
      "Epoch [184/200] Batch 0/1875                       Loss D: 30.8717, loss G: -16.6654\n",
      "Epoch [185/200] Batch 0/1875                       Loss D: 34.8242, loss G: -16.7834\n",
      "Epoch [186/200] Batch 0/1875                       Loss D: 38.3628, loss G: -17.4337\n",
      "Epoch [187/200] Batch 0/1875                       Loss D: 35.5267, loss G: -18.5936\n",
      "Epoch [188/200] Batch 0/1875                       Loss D: 40.5873, loss G: -19.1712\n",
      "Epoch [189/200] Batch 0/1875                       Loss D: 39.5615, loss G: -18.7421\n",
      "Epoch [190/200] Batch 0/1875                       Loss D: 32.1411, loss G: -15.2393\n",
      "Epoch [191/200] Batch 0/1875                       Loss D: 41.3226, loss G: -19.0359\n",
      "Epoch [192/200] Batch 0/1875                       Loss D: 31.4667, loss G: -15.6848\n",
      "Epoch [193/200] Batch 0/1875                       Loss D: 32.0117, loss G: -15.8373\n",
      "Epoch [194/200] Batch 0/1875                       Loss D: 30.8912, loss G: -12.2634\n",
      "Epoch [195/200] Batch 0/1875                       Loss D: 39.7794, loss G: -16.5381\n",
      "Epoch [196/200] Batch 0/1875                       Loss D: 35.5991, loss G: -18.4426\n",
      "Epoch [197/200] Batch 0/1875                       Loss D: 32.9302, loss G: -18.2147\n",
      "Epoch [198/200] Batch 0/1875                       Loss D: 35.0243, loss G: -18.8438\n",
      "Epoch [199/200] Batch 0/1875                       Loss D: 32.6355, loss G: -18.2130\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        # Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        dis_real = dis(real).view(-1)\n",
    "        dis_fake = dis(fake).view(-1)\n",
    "\n",
    "        lossD = -torch.sum((torch.log(dis_real) + \n",
    "                            torch.log(1 - dis_fake)))\n",
    "\n",
    "        dis.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_dis.step()\n",
    "\n",
    "        # Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = dis(fake).view(-1)\n",
    "        lossG = torch.sum(torch.log(1 - output))\n",
    "\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after training\n",
    "torch.save(gen.state_dict(), \"gan_after_training.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
